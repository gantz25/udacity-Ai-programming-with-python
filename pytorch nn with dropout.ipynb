{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55240a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets,transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627ed013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5318f6d59f584e3285ed6f36de074b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d55a7edc9324ab18176993b55c8796b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4be36411c1407aa49a7339cc465828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa31cda767804b3cb6b3786f74b4df29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/mohamed/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ca7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,optim\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95644406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = f.relu(self.fc2(x))\n",
    "        x = f.relu(self.fc3(x))\n",
    "        x = f.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecd251",
   "metadata": {},
   "source": [
    "Now you should create your network and train it. First you'll want to define the criterion (something like nn.CrossEntropyLoss or nn.NLLLoss) and the optimizer (typically optim.SGD or optim.Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2902aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=classifier()\n",
    "criterion=nn.NLLLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b7e4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.37574160013244606\n",
      "Training loss: 0.3366376662622891\n",
      "Training loss: 0.3121396289117682\n",
      "Training loss: 0.2911254740647797\n",
      "Training loss: 0.27483169110154293\n",
      "Training loss: 0.26433746637438915\n",
      "Training loss: 0.2502300455086013\n",
      "Training loss: 0.23867582044462915\n",
      "Training loss: 0.2276772339063794\n",
      "Training loss: 0.21719923532053606\n"
     ]
    }
   ],
   "source": [
    "epoch=10\n",
    "for i in range(10):\n",
    "    lose=0\n",
    "    for im,la in trainloader:\n",
    "        log_p=model(im)\n",
    "        loss=criterion(log_p,la)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lose+=loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {lose/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fdb5ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.2362e-02, 1.7374e-04, 7.1703e-01, 8.9311e-03, 7.9782e-02, 7.3554e-07,\n",
       "         1.5162e-01, 9.1045e-08, 9.3609e-05, 1.5301e-06]],\n",
       "       grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[1]\n",
    "\n",
    "# TODO: Calculate the class probabilities (softmax) for img\n",
    "ps = torch.exp(model(img))\n",
    "ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7265d39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]])\n"
     ]
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Look at the most likely classes for the first 10 examples\n",
    "print(top_class[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cbcf512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30..  Training Loss: 0.526..  Test Loss: 0.451..  Test Accuracy: 0.832\n",
      "Epoch: 2/30..  Training Loss: 0.393..  Test Loss: 0.453..  Test Accuracy: 0.838\n",
      "Epoch: 3/30..  Training Loss: 0.357..  Test Loss: 0.392..  Test Accuracy: 0.854\n",
      "Epoch: 4/30..  Training Loss: 0.333..  Test Loss: 0.380..  Test Accuracy: 0.862\n",
      "Epoch: 5/30..  Training Loss: 0.316..  Test Loss: 0.366..  Test Accuracy: 0.869\n",
      "Epoch: 6/30..  Training Loss: 0.302..  Test Loss: 0.355..  Test Accuracy: 0.873\n",
      "Epoch: 7/30..  Training Loss: 0.290..  Test Loss: 0.353..  Test Accuracy: 0.877\n",
      "Epoch: 8/30..  Training Loss: 0.281..  Test Loss: 0.373..  Test Accuracy: 0.873\n",
      "Epoch: 9/30..  Training Loss: 0.273..  Test Loss: 0.378..  Test Accuracy: 0.869\n",
      "Epoch: 10/30..  Training Loss: 0.263..  Test Loss: 0.370..  Test Accuracy: 0.878\n",
      "Epoch: 11/30..  Training Loss: 0.258..  Test Loss: 0.369..  Test Accuracy: 0.875\n",
      "Epoch: 12/30..  Training Loss: 0.254..  Test Loss: 0.388..  Test Accuracy: 0.875\n",
      "Epoch: 13/30..  Training Loss: 0.245..  Test Loss: 0.364..  Test Accuracy: 0.880\n",
      "Epoch: 14/30..  Training Loss: 0.240..  Test Loss: 0.382..  Test Accuracy: 0.877\n",
      "Epoch: 15/30..  Training Loss: 0.237..  Test Loss: 0.394..  Test Accuracy: 0.877\n",
      "Epoch: 16/30..  Training Loss: 0.236..  Test Loss: 0.391..  Test Accuracy: 0.872\n",
      "Epoch: 17/30..  Training Loss: 0.225..  Test Loss: 0.382..  Test Accuracy: 0.876\n",
      "Epoch: 18/30..  Training Loss: 0.222..  Test Loss: 0.377..  Test Accuracy: 0.879\n",
      "Epoch: 19/30..  Training Loss: 0.215..  Test Loss: 0.387..  Test Accuracy: 0.880\n",
      "Epoch: 20/30..  Training Loss: 0.217..  Test Loss: 0.382..  Test Accuracy: 0.878\n",
      "Epoch: 21/30..  Training Loss: 0.213..  Test Loss: 0.380..  Test Accuracy: 0.877\n",
      "Epoch: 22/30..  Training Loss: 0.208..  Test Loss: 0.402..  Test Accuracy: 0.880\n",
      "Epoch: 23/30..  Training Loss: 0.200..  Test Loss: 0.385..  Test Accuracy: 0.880\n",
      "Epoch: 24/30..  Training Loss: 0.201..  Test Loss: 0.423..  Test Accuracy: 0.875\n",
      "Epoch: 25/30..  Training Loss: 0.198..  Test Loss: 0.413..  Test Accuracy: 0.880\n",
      "Epoch: 26/30..  Training Loss: 0.197..  Test Loss: 0.418..  Test Accuracy: 0.879\n",
      "Epoch: 27/30..  Training Loss: 0.202..  Test Loss: 0.408..  Test Accuracy: 0.881\n",
      "Epoch: 28/30..  Training Loss: 0.190..  Test Loss: 0.447..  Test Accuracy: 0.884\n",
      "Epoch: 29/30..  Training Loss: 0.187..  Test Loss: 0.422..  Test Accuracy: 0.881\n",
      "Epoch: 30/30..  Training Loss: 0.183..  Test Loss: 0.415..  Test Accuracy: 0.881\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = classifier()\n",
    "criterion = nn.NLLLoss(reduction='sum')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    tot_train_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        tot_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        tot_test_loss = 0\n",
    "        test_correct = 0  # Number of correct predictions on the test set\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                tot_test_loss += loss.item()\n",
    "\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                test_correct += equals.sum().item()\n",
    "\n",
    "        # Get mean loss to enable comparison between train and test sets\n",
    "        train_loss = tot_train_loss / len(trainloader.dataset)\n",
    "        test_loss = tot_test_loss / len(testloader.dataset)\n",
    "\n",
    "        # At completion of epoch\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss),\n",
    "              \"Test Accuracy: {:.3f}\".format(test_correct / len(testloader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b41ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc1959",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference \n",
    "model.eval()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "\n",
    "ps = torch.exp(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
